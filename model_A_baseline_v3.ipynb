{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import utils\n",
    "\n",
    "# SKLearn related imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from lightgbm import LGBMRegressor  # or any other estimator\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataset 1\n",
    "df1 = pd.read_csv(\"train/chain_campaigns.csv\")\n",
    "# Convert dates and compute duration\n",
    "df1['start_date'] = pd.to_datetime(df1['start_date'])\n",
    "df1['end_date'] = pd.to_datetime(df1['end_date'])\n",
    "df1['duration_days'] = (df1['end_date'] - df1['start_date']).dt.days + 1\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf00a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataset 2\n",
    "df2 = pd.read_csv(\"train/product_prices_leaflets.csv\")\n",
    "df2['time_key'] = pd.to_datetime(df2['time_key'].astype(str), format='%Y%m%d') #convert time_key to datetime\n",
    "#df2 = df2.set_index(['sku','time_key']).sort_index()\n",
    "\n",
    "#drop negative discount values\n",
    "neg_count = (df2['discount'] < 0).sum()\n",
    "print(f\"Dropping {neg_count} rows with negative discount\")\n",
    "\n",
    "# Filter them out\n",
    "df2 = df2[df2['discount'] >= 0].copy()\n",
    "\n",
    "df2['effective_price'] = df2['pvp_was'] * (1 - df2['discount'])\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(columns=['leaflet']) #remove \"leaflet\" column for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203afa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df2.duplicated().sum()\n",
    "print(f\"Exact duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350cb588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### remove duplicates from df2\n",
    "df2_cleaned = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###dataset 3\n",
    "df3 = pd.read_csv(\"train/product_structures_sales.csv\")\n",
    "\n",
    "df3['time_key'] = pd.to_datetime(df3['time_key'].astype(str), format='%Y%m%d') #convert time_key to datetime\n",
    "#df3 = df3.set_index(['time_key','sku']).sort_index()\n",
    "\n",
    "#drop negative quantity values\n",
    "neg_count = (df3['quantity'] < 0).sum()\n",
    "print(f\"Dropping {neg_count} rows with negative quantity\")\n",
    "\n",
    "# Filter them out\n",
    "df3 = df3[df3['quantity'] >= 0].copy()\n",
    "\n",
    "#transform structure_levels into categorical variables\n",
    "structure_cols = [\n",
    "    'structure_level_1',\n",
    "    'structure_level_2',\n",
    "    'structure_level_3',\n",
    "    'structure_level_4'\n",
    "]\n",
    "\n",
    "for col in structure_cols:\n",
    "    df3[col] = df3[col].astype('category')\n",
    "\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Only get the competitor A and chain prices (eliminate competitor B from df2)\n",
    "df2_comp_A = df2_cleaned[df2_cleaned['competitor'] != 'competitorB'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d24aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(\n",
    "    df2_comp_A,\n",
    "    df3,\n",
    "    on=['sku','time_key'],\n",
    "    how='left' #keep all df2 skus\n",
    ")\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_merged = df_merged.duplicated().sum()\n",
    "print(f\"Exact duplicate rows: {duplicates_merged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ee352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['competitor'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### separate data chain vs competitor A\n",
    "\n",
    "# 1. Split out the two series\n",
    "df_chain = df_merged[df_merged['competitor'] == 'chain'].copy()\n",
    "df_A = df_merged[df_merged['competitor'] == 'competitorA'].copy()\n",
    "\n",
    "# 2. Drop the old competitor column and rename the remaining columns\n",
    "df_chain = df_chain.drop(columns='competitor').rename(columns={\n",
    "    'pvp_was':      'pvp_was_chain',\n",
    "    'discount':     'discount_chain',\n",
    "    'flag_promo':   'flag_promo_chain',\n",
    "    'quantity':   'quantity_chain',\n",
    "    'effective_price': 'effective_price_chain'  # chain doesn't have effective_price target\n",
    "})\n",
    "\n",
    "df_A = df_A.drop(columns='competitor').rename(columns={\n",
    "    'pvp_was':        'pvp_was_A',\n",
    "    'discount':       'discount_A',\n",
    "    'flag_promo':     'flag_promo_A',\n",
    "    'quantity':   'quantity_A',\n",
    "    'effective_price': 'effective_price_A'\n",
    "})\n",
    "\n",
    "# 3. Merge back together on sku & time_key\n",
    "df_wide_prices = pd.merge(\n",
    "    df_chain,\n",
    "    df_A,\n",
    "    on=['sku','time_key'],\n",
    "    how='inner',  # or 'outer' if you want to keep rows missing one side\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ddb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_prices.shape ## ver se os valores desta nova dataframe fazem sentido --> fiquei aqui; se sim, desenvolver modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_wide_prices.drop(columns=['effective_price_chain', 'discount_A', 'quantity_A', 'structure_level_4_y', 'structure_level_3_y', 'structure_level_2_y', 'structure_level_1_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'structure_level_1_x': 'structure_level_1',\n",
    "    'structure_level_2_x': 'structure_level_2',\n",
    "    'structure_level_3_x': 'structure_level_3',\n",
    "    'structure_level_4_x': 'structure_level_4'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae607e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['time_key'].dt.month        \n",
    "df['day_of_week'] = df['time_key'].dt.weekday      \n",
    "df['day_of_month']= df['time_key'].dt.day         \n",
    "df['year'] = df['time_key'].dt.year\n",
    "df['is_weekend']  = (df['time_key'].dt.weekday >= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371635fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Impute via augmented masking\n",
    "\n",
    "df = df.sort_values(['sku','time_key']).reset_index(drop=True)\n",
    "\n",
    "# 2) Compute lag features on pvp_was_A\n",
    "df['price_lag_1_A'] = df.groupby('sku')['pvp_was_A'].shift(1)\n",
    "df['price_roll_mean_7_A'] = (\n",
    "    df.groupby('sku')['pvp_was_A']\n",
    "      .shift(1)\n",
    "      .rolling(7)\n",
    "      .mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# 3) Build your imputation rule: category‐mean spread\n",
    "df_warm = df.dropna(subset=['price_lag_1_A'])\n",
    "df_warm['spread'] = df_warm['price_lag_1_A'] - df_warm['pvp_was_chain']\n",
    "cat_spread   = df_warm.groupby('structure_level_2')['spread'].mean().to_dict()\n",
    "global_spread = df_warm['spread'].mean()\n",
    "\n",
    "def impute_lags(row):\n",
    "    sp   = cat_spread.get(row['structure_level_2'], global_spread)\n",
    "    base = row['pvp_was_chain'] + sp\n",
    "    return base, base\n",
    "\n",
    "# 4) Create the augmentation mask on 10% of warm rows\n",
    "rng      = np.random.default_rng(42)\n",
    "warm_idx = df[df['price_lag_1_A'].notna()].index\n",
    "mask_idx = rng.choice(warm_idx, size=int(len(warm_idx)*0.1), replace=False)\n",
    "\n",
    "# 5) Initialize the flag\n",
    "df['is_imputed_A'] = 0\n",
    "\n",
    "# 6) Apply masking: overwrite lag features & set flag\n",
    "df.loc[mask_idx, 'is_imputed_A'] = 1\n",
    "for idx in mask_idx:\n",
    "    lag1, roll7 = impute_lags(df.loc[idx])\n",
    "    df.at[idx, 'price_lag_1_A']       = lag1\n",
    "    df.at[idx, 'price_roll_mean_7_A'] = roll7\n",
    "\n",
    "# Now:\n",
    "#  - `price_lag_1_A` and `price_roll_mean_7_A` are real for 90% of warm SKUs, imputed for 10%\n",
    "#  - `is_imputed_A` = 1 marks the imputed cases\n",
    "#  - cold-start SKUs (NaN lags) remain NaN here and you’ll impute them at inference\n",
    "\n",
    "# 7) Ready for time‐split and model training\n",
    "\n",
    "feature_cols = [\n",
    "    'sku', 'pvp_was_chain', 'discount_chain',\n",
    "    'flag_promo_chain', 'structure_level_4', 'structure_level_3',\n",
    "        'structure_level_2', 'structure_level_1', 'quantity_chain', 'flag_promo_A', 'month', 'day_of_week', 'day_of_month', 'year', 'price_lag_1_A','price_roll_mean_7_A']\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['effective_price_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f607dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_cols = [\n",
    "#    'sku', 'pvp_was_chain', 'discount_chain',\n",
    "#    'flag_promo_chain', 'structure_level_4', 'structure_level_3',\n",
    "#    'structure_level_2', 'structure_level_1', 'quantity_chain', 'flag_promo_A', 'month', 'day_of_week', 'day_of_month', 'year']\n",
    "\n",
    "#X = df[feature_cols]\n",
    "#y = df['effective_price_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8da2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train test split time-based & split into X and y (train and test sets)\n",
    "split = int(len(df) * 0.8)\n",
    "\n",
    "train_X, test_X = X.iloc[:split], X.iloc[split:]\n",
    "train_y, test_y = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "print(f\"Train from {df['time_key'].iloc[0].date()} to {df['time_key'].iloc[split-1].date()}\")\n",
    "print(f\"Test  from {df['time_key'].iloc[split].date()} to {df['time_key'].iloc[-1].date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['sku', 'structure_level_1', 'structure_level_2', 'structure_level_3', 'structure_level_4', 'flag_promo_chain', 'flag_promo_A', 'month', 'day_of_week']\n",
    "numeric_cols     = ['pvp_was_chain', 'discount_chain', 'quantity_chain', 'year', 'day_of_month', 'price_lag_1_A', 'price_roll_mean_7_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290140d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fff680",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols),\n",
    "    (\"num\", RobustScaler(), numeric_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model pipeline\n",
    "\n",
    "def model_pipeline(train_X, test_X, train_y, test_y, n_estimators=100, random_state=42):\n",
    "\n",
    "    clf = Pipeline([\n",
    "        ('preproc',preprocessor),\n",
    "        ('regressor', LGBMRegressor(n_estimators=n_estimators, random_state=random_state))])\n",
    "\n",
    "    clf.fit(train_X, train_y)\n",
    "\n",
    "    y_pred = clf.predict(test_X)\n",
    "    \n",
    "    mae = mean_absolute_error(test_y, y_pred)\n",
    "    print(f\"MAE: {mae}\")\n",
    "    \n",
    "    mape = (abs(test_y - y_pred) / test_y).mean() * 100\n",
    "    print(f\"MAPE: {mape:.1f}%\")\n",
    "\n",
    "    return clf, y_pred, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model evaluation (MAE)\n",
    "model, preds, mae, mape = model_pipeline(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33efce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#serialize columns\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "with open('columns_A_1.json', 'w') as fh:\n",
    "    json.dump(train_X.columns.tolist(), fh)\n",
    "    \n",
    "\n",
    "#serialize dtypes of the columns\n",
    "\n",
    "with open('dtypes_A_1.pickle', 'wb') as fh:\n",
    "    pickle.dump(train_X.dtypes, fh)\n",
    "    \n",
    "    \n",
    "#serialize fitted pipeline\n",
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'pipeline_A_1.pickle') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s06",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
